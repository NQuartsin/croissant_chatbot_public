{
  "name": "HuggingFaceH4/ultrachat_200k",
  "creators": "HuggingFaceH4",
  "description": "\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for UltraChat 200k\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a heavily filtered version of the UltraChat dataset and was used to train Zephyr-7B-\u03b2, a state of the art 7b chat model.\nThe original datasets consists of 1.4M dialogues generated by ChatGPT and spanning a wide range of topics. To create UltraChat 200k, we applied the following logic:\n\nSelection of a subset of data for faster supervised fine tuning.\nTruecasing of the dataset, as we observed around 5% of\u2026 See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k.",
  "license": "mit",
  "url": "",
  "publisher": "",
  "version": "",
  "keywords": "",
  "date_modified": "",
  "date_created": "",
  "date_published": "",
  "cite_as": "@misc{ding2023enhancing, \n      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \n      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou}, \n      year={2023}, \n      eprint={2305.14233}, \n      archivePrefix={arXiv}, \n      primaryClass={cs.CL} \n}",
  "task": "text-generation",
  "modality": "text",
  "in_language": "en"
}