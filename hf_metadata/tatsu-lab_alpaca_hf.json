{
  "name": "tatsu-lab/alpaca",
  "creators": "tatsu-lab",
  "description": "\n\t\n\t\t\n\t\tDataset Card for Alpaca\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nAlpaca is a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better.\nThe authors built on the data generation pipeline from Self-Instruct framework and made the following modifications:\n\nThe text-davinci-003 engine to generate the instruction data instead\u2026 See the full description on the dataset page: https://huggingface.co/datasets/tatsu-lab/alpaca.",
  "license": "cc-by-nc-4.0",
  "url": "",
  "publisher": "",
  "version": "",
  "keywords": "instruction-finetuning",
  "date_modified": "",
  "date_created": "",
  "date_published": "",
  "cite_as": "@misc{alpaca,\n  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto }, \n  title = {Stanford Alpaca: An Instruction-following LLaMA model}, \n  year = {2023}, \n  publisher = {GitHub}, \n  journal = {GitHub repository}, \n  url = {{https://github.com/tatsu-lab/stanford_alpaca}}, \n}",
  "task": "text-generation",
  "modality": "text",
  "in_language": "en"
}