{
  "@context": {
    "@language": "en",
    "@vocab": "https://schema.org/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http://mlcommons.org/croissant/",
    "rai": "http://mlcommons.org/croissant/RAI/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http://purl.org/dc/terms/",
    "examples": {
      "@id": "cr:examples",
      "@type": "@json"
    },
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https://schema.org/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "name": "uonlp/CulturaX",
  "description": "CulturaX, a groundbreaking dataset, offers an enormous collection of text data in 167 languages, totaling 6.3 trillion tokens. The dataset undergoes a rigorous cleaning and deduplication process to ensure the best quality for model training. With CulturaX, researchers and developers can now train large language models in a multilingual context, paving the way for more inclusive and effective AI models.",
  "conformsTo": "http://mlcommons.org/croissant/1.0",
  "citeAs": "@inproceedings{nguyen-etal-2024-culturax,     title = \"{C}ultura{X}: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages\",     author = \"Nguyen, Thuat  and       Nguyen, Chien Van  and       Lai, Viet Dac  and       Man, Hieu  and       Ngo, Nghia Trung  and       Dernoncourt, Franck  and       Rossi, Ryan A.  and       Nguyen, Thien Huu\",     editor = \"Calzolari, Nicoletta  and       Kan, Min-Yen  and       Hoste, Veronique  and       Lenci, Alessandro  and       Sakti, Sakriani  and       Xue, Nianwen\",     booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",     month = may,     year = \"2024\",     address = \"Torino, Italia\",     publisher = \"ELRA and ICCL\",     url = \"https://aclanthology.org/2024.lrec-main.377\",     pages = \"4226--4237\",     abstract = \"Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.\", }",
  "creator": "uonlp",
  "dateCreated": "2023-09-04",
  "dateModified": "2024-12-16",
  "datePublished": "2023-09-04",
  "inLanguage": "af, als, am, an, ar, arz, as, ast, av, az, azb, ba, bar, bcl, be, bg, bh, bn, bo, bpy, br, bs, bxr, ca, cbk, ce, ceb, ckb, cs, cv, cy, da, de, dsb, dv, el, eml, en, eo, es, et, eu, fa, fi, fr, frr, fy, ga, gd, gl, gn, gom, gu, he, hi, hr, hsb, ht, hu, hy, ia, id, ie, ilo, io, is, it, ja, jbo, jv, ka, kk, km, kn, ko, krc, ku, kv, kw, ky, la, lb, lez, li, lmo, lo, lrc, lt, lv, mai, mg, mhr, min, mk, ml, mn, mr, mrj, ms, mt, mwl, my, myv, mzn, nah, nap, nds, ne, new, nl, nn, no, oc, or, os, pa, pam, pl, pms, pnb, ps, pt, qu, rm, ro, ru, rue, sa, sah, scn, sd, sh, si, sk, sl, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, tyv, ug, uk, ur, uz, vec, vi, vls, vo, wa, war, wuu, xal, xmf, yi, yo, yue, zh",
  "keywords": "language-modeling, masked-language-modeling, LLM, Dataset for Training",
  "license": "mC4, OSCAR",
  "url": "https://huggingface.co/datasets/uonlp/CulturaX",
  "task": "text-generation, fill-mask",
  "modality": "text"
}