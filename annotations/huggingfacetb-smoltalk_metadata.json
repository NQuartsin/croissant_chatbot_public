{
  "@context": {
    "@language": "en",
    "@vocab": "https://schema.org/",
    "citeAs": "cr:citeAs",
    "column": "cr:column",
    "conformsTo": "dct:conformsTo",
    "cr": "http://mlcommons.org/croissant/",
    "rai": "http://mlcommons.org/croissant/RAI/",
    "data": {
      "@id": "cr:data",
      "@type": "@json"
    },
    "dataType": {
      "@id": "cr:dataType",
      "@type": "@vocab"
    },
    "dct": "http://purl.org/dc/terms/",
    "examples": {
      "@id": "cr:examples",
      "@type": "@json"
    },
    "extract": "cr:extract",
    "field": "cr:field",
    "fileProperty": "cr:fileProperty",
    "fileObject": "cr:fileObject",
    "fileSet": "cr:fileSet",
    "format": "cr:format",
    "includes": "cr:includes",
    "isLiveDataset": "cr:isLiveDataset",
    "jsonPath": "cr:jsonPath",
    "key": "cr:key",
    "md5": "cr:md5",
    "parentField": "cr:parentField",
    "path": "cr:path",
    "recordSet": "cr:recordSet",
    "references": "cr:references",
    "regex": "cr:regex",
    "repeated": "cr:repeated",
    "replace": "cr:replace",
    "sc": "https://schema.org/",
    "separator": "cr:separator",
    "source": "cr:source",
    "subField": "cr:subField",
    "transform": "cr:transform"
  },
  "@type": "sc:Dataset",
  "name": "HuggingFaceTB/smoltalk",
  "description": "The SmolTalk dataset, engineered by HuggingFaceTB, is a synthetic dataset specifically designed for the supervised finetuning (SFT) of large language models (LLMs). This dataset, consisting of 1 million samples, was instrumental in the creation of the SmolLM2-Instruct family of models. During the development of SmolLM2, it was discovered that models finetuned on public SFT datasets performed suboptimally compared to models trained on exclusive instruction datasets. To address this issue, the SmolTalk dataset was developed, achieving better performance for models finetuned on it. More details about this dataset can be found in the paper at: https://arxiv.org/abs/2502.02737",
  "conformsTo": "http://mlcommons.org/croissant/1.0",
  "citeAs": "@misc{allal2025smollm2smolgoesbig,       title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},        author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart\u00edn Bl\u00e1zquez and Guilherme Penedo and Lewis Tunstall and Andr\u00e9s Marafioti and Hynek Kydl\u00ed\u010dek and Agust\u00edn Piqueres Lajar\u00edn and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl\u00e9mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},       year={2025},       eprint={2502.02737},       archivePrefix={arXiv},       primaryClass={cs.CL},       url={https://arxiv.org/abs/2502.02737},  }",
  "creator": "HuggingFaceTB",
  "dateCreated": "2024-11-17",
  "dateModified": "2025-02-10",
  "datePublished": "2024-11-17",
  "inLanguage": "en",
  "keywords": "Synthetic Datasets, Supervised Finetuning, SmolLM2",
  "url": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk",
  "task": "Supervised Fine-tuning (SFT) for Language Models",
  "modality": "tabular, text"
}